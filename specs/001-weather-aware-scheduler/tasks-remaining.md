# Tasks: Weather-Aware Scheduler - Remaining Work to Production Ready

**Feature**: Weather-Aware Scheduler with LangGraph State Machine
**Current Status**: ğŸŸ¡ **83% Complete** - 111/125 tests passing (14 failures, 37 skipped)
**Goal**: ğŸ¯ **100% Usable** - All tests pass after setting .env, system production-ready
**Last Updated**: 2025-10-26

---

## ğŸ“Š Current Implementation Status

### âœ… Completed Components (Ready to Use)
- [x] LangGraph StateGraph with 6 nodes (intent_and_slots, check_weather, find_free_slot, confirm_or_adjust, create_event, error_recovery)
- [x] All contract tests passing (22/22) - Tool signatures correct
- [x] Mock tools with deterministic behavior (weather + calendar)
- [x] Natural language parser with time/date/location extraction
- [x] Weather-aware decision logic (HIGH/MODERATE/LOW risk)
- [x] Conflict detection and alternative slot finding
- [x] Retry logic with graceful degradation
- [x] CLI with Typer + Rich formatting
- [x] Pydantic v2 models (no deprecation warnings)
- [x] Unit tests for parsers, validators, formatters (all passing)
- [x] Integration tests for sunny path, rainy adjustment, conflict resolution (35/46 passing)

### âš ï¸ Remaining Issues (14 Test Failures)

**Category 1: Degradation Notes Not Propagating** (4 failures)
- Test expects degradation notes in event.notes when services fail
- Error recovery node sets degradation_notes but create_event doesn't use them
- **Files**: tests/integration/test_service_failure.py

**Category 2: Clarification Workflow Incomplete** (5 failures)
- Tests expect clarification_needed to be set with examples
- Error recovery node generates clarification but workflow incomplete
- **Files**: tests/integration/test_missing_slot.py

**Category 3: Unit Test Signature Mismatch** (5 failures)
- Tests expect old find_free_slot signature (datetime return)
- Need to update tests to use new contract (dict return)
- **Files**: tests/unit/test_mock_tools.py, tests/unit/test_calendar_tools.py

### ğŸ”µ Optional: Multi-Agent Framework (37 Skipped Tests)
- Microsoft Agent Framework tests skipped (auxiliary feature)
- Only needed for Teams/WeChat/LINE integrations
- Not blocking for core functionality

---

## ğŸ¯ Remaining Tasks (Organized by Completion Goal)

### Phase 1: Fix Failing Tests (Priority: CRITICAL - Blocks Production)

**Goal**: All 125 active tests pass (14 â†’ 0 failures)
**Duration**: 2-3 hours
**Independent Test**: Run `pytest tests/ --no-cov` and verify 125/125 pass

---

#### Category A: Service Failure Degradation Notes (T001-T004)

**Problem**: When weather/calendar services fail after retries, degradation notes aren't appearing in event.notes

- [ ] **T001** `[P]` Fix degradation notes propagation in create_event_node
  - **File**: `src/graph/nodes.py` (create_event_node function, lines 258-343)
  - **Issue**: Line 294-296 checks for degradation_notes but doesn't include them in event notes
  - **Fix**:
    ```python
    # Current (line 289-297):
    notes_parts = []
    if policy_decision.get("notes"):
        notes_parts.append(policy_decision["notes"])

    # Add degradation notes if services failed
    if state.get("degradation_notes"):
        notes_parts.extend(state["degradation_notes"])

    combined_notes = "\n".join(notes_parts) if notes_parts else None
    ```
  - **Test Files**:
    - tests/integration/test_service_failure.py::test_weather_service_failure_retries_once_then_degrades
    - tests/integration/test_service_failure.py::test_calendar_service_failure_retries_once_then_degrades
    - tests/integration/test_service_failure.py::test_both_services_fail_creates_event_with_warnings
    - tests/integration/test_service_failure.py::test_service_failure_notes_are_actionable
  - **Verification**: `pytest tests/integration/test_service_failure.py -v`
  - **Expected**: 5/5 tests pass (currently 1/5 pass)

---

#### Category B: Clarification Workflow (T005-T009)

**Problem**: Error recovery node generates clarification_needed but it's not being returned to user properly

- [ ] **T005** Fix clarification response in error_recovery_node
  - **File**: `src/graph/nodes.py` (error_recovery_node function, lines 346-467)
  - **Issue**: Clarification is set but graph doesn't return to user
  - **Root Cause**: Need to check graph edges from error_recovery_node
  - **Investigation**:
    1. Check `src/graph/edges.py` conditional_edge_from_error function
    2. Verify error_recovery routes to END when clarification_needed is set
    3. Ensure clarification_needed persists in final state
  - **Test Files**:
    - tests/integration/test_missing_slot.py::test_missing_time_and_location_triggers_clarification
    - tests/integration/test_missing_slot.py::test_clarification_with_complete_info_creates_event
  - **Verification**: `pytest tests/integration/test_missing_slot.py::test_missing_time_and_location_triggers_clarification -v`

- [ ] **T006** Add clarification_count tracking to SchedulerState
  - **File**: `src/models/state.py`
  - **Issue**: Tests expect clarification_count field but it doesn't exist
  - **Fix**: Add `clarification_count: int` to SchedulerState TypedDict (default 0)
  - **Update**: Intent_and_slots_node and error_recovery_node to increment clarification_count
  - **Test Files**:
    - tests/integration/test_missing_slot.py::test_one_shot_clarification_strategy
    - tests/integration/test_missing_slot.py::test_missing_duration_uses_default
  - **Verification**: Check that clarification_count field exists and increments

- [ ] **T007** Implement one-shot clarification strategy
  - **File**: `src/graph/nodes.py` (error_recovery_node)
  - **Issue**: Tests expect exactly 1 clarification attempt before giving format examples
  - **Current**: Error recovery retries up to 2 times
  - **Fix**:
    - First failure (clarification_count=0): Ask for missing fields with examples
    - Second failure (clarification_count=1): Provide full format example and end
  - **Test Files**:
    - tests/integration/test_missing_slot.py::test_one_shot_clarification_strategy
    - tests/integration/test_missing_slot.py::test_clarification_includes_format_examples
  - **Verification**: `pytest tests/integration/test_missing_slot.py -v`
  - **Expected**: 5/5 tests pass (currently 0/5 pass)

---

#### Category C: Unit Test Signature Updates (T008-T012)

**Problem**: Unit tests expect old find_free_slot signature (returns datetime) but implementation now returns dict

- [ ] **T008** `[P]` Update remaining test_mock_tools.py find_free_slot tests
  - **File**: `tests/unit/test_mock_tools.py`
  - **Tests to Fix**:
    - test_find_free_slot_respects_search_window (line 154)
    - test_find_free_slot_various_durations_15min (line 173)
    - test_find_free_slot_various_durations_120min (line 188)
  - **Pattern**:
    ```python
    # Old: free_slot = tool.find_free_slot(...)
    # New: result = tool.find_free_slot(...)
    #      free_slot = result["next_available"]
    ```
  - **Verification**: `pytest tests/unit/test_mock_tools.py::TestMockCalendarTool -v`
  - **Expected**: All calendar tool tests pass

- [ ] **T009** `[P]` Update test_calendar_tools.py find_free_slot test
  - **File**: `tests/unit/test_calendar_tools.py`
  - **Test**: test_find_free_slot_returns_alternatives (line 101)
  - **Fix**: Update to expect dict with candidates list
  - **Verification**: `pytest tests/unit/test_calendar_tools.py::test_find_free_slot_returns_alternatives -v`

- [ ] **T010** `[P]` Update test_combined_avoidance_strategy
  - **File**: `tests/unit/test_mock_tools.py`
  - **Test**: TestMockToolIntegration::test_combined_avoidance_strategy (line 261)
  - **Issue**: Uses find_free_slot expecting datetime return
  - **Fix**: Update to use result["next_available"]
  - **Verification**: `pytest tests/unit/test_mock_tools.py::TestMockToolIntegration::test_combined_avoidance_strategy -v`

---

### Phase 2: Environment Setup Documentation (Priority: HIGH - Enables Testing)

**Goal**: Users can set .env and run all tests successfully
**Duration**: 30 minutes

- [ ] **T011** Create comprehensive .env.example with all required variables
  - **File**: `.env.example`
  - **Content**:
    ```bash
    # OpenAI API Key (for LLM-based parsing - optional in mock mode)
    OPENAI_API_KEY=sk-...

    # Azure OpenAI (alternative to OpenAI)
    AZURE_OPENAI_API_KEY=...
    AZURE_OPENAI_ENDPOINT=https://....openai.azure.com/
    AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini

    # Agent Mode (rule_engine = LangGraph, multi_agent = Microsoft Framework)
    AGENT_MODE=rule_engine

    # ASCII Output (for Windows console compatibility)
    ASCII_ONLY=false

    # Mock Mode (true = no API calls, false = real weather/calendar APIs)
    MOCK_MODE=true
    ```
  - **Verification**: Copy to .env, verify tests run without API errors

- [ ] **T012** Update README.md with quickstart instructions
  - **File**: `README.md`
  - **Add sections**:
    1. Quick Start (3 steps: clone, copy .env, run test)
    2. Running Tests (pytest command with options)
    3. Using CLI (example commands with expected output)
    4. Environment Variables (reference .env.example)
  - **Example**:
    ```bash
    # Quick Start
    1. Copy .env.example to .env
    2. Set MOCK_MODE=true for testing without API keys
    3. Run: uv run pytest tests/ --no-cov

    # CLI Usage
    weather-scheduler schedule "Friday 2pm Taipei meet Alice 60min"
    weather-scheduler visualize --output docs/diagrams
    ```
  - **Verification**: New user can follow README and run tests successfully

---

### Phase 3: Multi-Agent Framework (Priority: LOW - Optional Enhancement)

**Goal**: Enable Teams/WeChat/LINE integrations (only if needed)
**Duration**: 4-5 hours
**Status**: 37 tests skipped (not blocking)

- [ ] **T013** Complete multi-agent framework tests (OPTIONAL)
  - **Files**: tests/agents/test_parser_agent.py, tests/agents/test_calendar_agent.py, tests/integration/test_us1_simple_scheduling.py
  - **Note**: Only needed if you plan to use Teams/WeChat/LINE integrations
  - **Skip if**: You only need CLI/API usage
  - **Verification**: `pytest tests/agents/ tests/integration/test_us1_simple_scheduling.py -v`
  - **Expected**: 37/37 agent tests pass

---

## ğŸ“‹ Task Execution Order

### Critical Path to Production (Must Complete)

```
T001 (Fix degradation notes)
  â†“
T005 (Fix clarification routing)
  â†“
T006 (Add clarification_count tracking)
  â†“
T007 (One-shot clarification)
  â†“
[T008, T009, T010] (Unit test fixes - can run in parallel)
  â†“
T011 (Create .env.example)
  â†“
T012 (Update README)
  â†“
âœ… PRODUCTION READY (125/125 tests pass)
```

### Optional Enhancement Path

```
T013 (Multi-agent tests) â†’ 162/162 tests pass (includes auxiliary features)
```

---

## ğŸ¯ Success Criteria

### Minimum Usable State (After T001-T012)
- âœ… All 125 active tests pass (0 failures)
- âœ… User can copy .env.example â†’ .env
- âœ… User can run `pytest tests/ --no-cov` successfully
- âœ… CLI works: `weather-scheduler schedule "Friday 2pm Taipei meet Alice 60min"`
- âœ… README provides clear setup instructions

### Full Feature Complete (After T013)
- âœ… All 162 tests pass (including multi-agent)
- âœ… Teams/WeChat/LINE integration ready
- âœ… Production deployment ready

---

## ğŸ”§ Development Commands

```bash
# Run all active tests (skip multi-agent)
uv run pytest tests/ --no-cov -k "not agents and not test_us1"

# Run only failing tests
uv run pytest tests/integration/test_service_failure.py tests/integration/test_missing_slot.py tests/unit/test_mock_tools.py -v

# Run specific test
uv run pytest tests/integration/test_service_failure.py::test_weather_service_failure_retries_once_then_degrades -v

# Check coverage
uv run pytest tests/ --cov=src --cov-report=term-missing

# Run CLI
uv run python -m src.cli.main schedule "Friday 2pm Taipei meet Alice 60min"
```

---

## ğŸ“ Notes

**User Context**: è·é›¢å¯ä»¥ä½¿ç”¨é‚„æœ‰å“ªäº›æœªå®Œæˆ (å¯ä»¥ä½¿ç”¨æ„æ€:å°‡ç’°å¢ƒè®Šæ•¸è¨­å®šåœ¨.envå¾Œ,å°±èƒ½é †åˆ©æ¸¬è©¦)

**Translation**: "How far from usable / what's remaining incomplete" (usable means: after setting environment variables in .env, testing works smoothly)

**Answer**: Only **12 tasks (T001-T012)** separate you from production-ready state. Most critical:
1. T001: Fix degradation notes (30 min)
2. T005-T007: Complete clarification workflow (1-1.5 hours)
3. T008-T010: Update unit tests (30 min)
4. T011-T012: Documentation (30 min)

**Total Estimated Time**: 2.5-3 hours to fully usable state.

**Current Progress**:
- Core functionality: âœ… 100% complete
- Test coverage: ğŸŸ¡ 89% (111/125 passing)
- Documentation: ğŸŸ¡ 70% complete
- Production ready: ğŸŸ¡ After T001-T012 completion

---

## ğŸš€ Quick Fix Priority

If you only have 1 hour:
1. **T001** - Fix degradation notes (enables 4 service failure tests)
2. **T008-T010** - Fix unit tests (enables 5 tool tests)
â†’ This gets you to 120/125 tests passing (96%)

If you have 3 hours:
1. Complete all T001-T012
â†’ This gets you to 125/125 tests passing (100%) + production ready
