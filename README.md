# Weather-Aware Scheduler

A lightweight, explainable scheduling system that parses natural language inputs, evaluates weather conditions and calendar conflicts, and produces clear, testable scheduling decisions with transparent reasoning.

## Features

- **Natural Language Input:** Schedule meetings by typing plain English
- **Weather-Aware:** Automatically detects rain risk and suggests adjustments
- **Conflict Detection:** Checks calendar availability and proposes alternatives
- **Transparent Decisions:** Every decision includes clear reasoning and notes
- **Offline-First:** Mock mode requires zero API keys (perfect for development)
- **Visualizable:** Generate flowcharts showing decision-making process
- **Test-Driven:** 100% tested with golden-path scenarios

## Quick Start

### Installation

```bash
pip install -e ".[dev]"
```

### Schedule a Meeting

```bash
weather-scheduler schedule "Friday 2pm Taipei meet Alice 60min"
```

**Output:**
```
âœ“ Event Created

Meeting scheduled in Taipei
Reason: No conflicts detected, weather conditions acceptable
```

### Generate Workflow Visualization

```bash
weather-scheduler visualize
```

This creates `graph/flow.mermaid` and `graph/flow.dot` showing the decision flow.

## User Stories

### âœ… User Story 1: Simple Schedule Creation (MVP)

**Goal:** Quickly schedule meetings with natural language

**Example:**
```bash
weather-scheduler schedule "Next Monday 10am review meeting 45min"
```

### âœ… User Story 2: Weather-Aware Scheduling

**Goal:** Get proactive weather-based suggestions

**Example:**
```bash
weather-scheduler schedule "Tomorrow afternoon Taipei coffee rain backup"
```

**Output:** System detects rain risk and suggests indoor venue or time shift

### âœ… User Story 3: Conflict Resolution

**Goal:** Automatically detect conflicts and propose alternatives

**Example:**
```bash
weather-scheduler schedule "Friday 3pm team sync 30min"
```

**Output:** 3 alternative time slots if conflict detected

### ğŸš§ User Story 4: Process Visualization

**Goal:** Understand decision flow through visual diagrams

**Example:**
```bash
weather-scheduler visualize
```

**Output:** Mermaid and DOT format diagrams

## System Architecture

### Decision Flow

```
Input â†’ Parse & Validate â†’ Check Weather â†’ Check Calendar
                                              â†“
                                      Conflict/Weather Risk?
                                              â†“
                                   Yes â† â†’ No
                                    â†“         â†“
                           Suggest Adjust   Create Event
                                    â†“         â†“
                                  Output Summary
```

### Components

- **Parser:** Natural language â†’ structured slot (city, datetime, duration, attendees)
- **Validator:** Ensure dates are valid and in future
- **Weather Tool:** Check rain probability (mock mode: keyword + time window rules)
- **Calendar Tool:** Check conflicts (mock mode: predefined blocked times)
- **Policy Engine:** Decide: create, adjust time, adjust place, or propose alternatives
- **Formatter:** Structured output â†’ Rich CLI display

### Technology Stack

- **LangGraph:** State machine orchestration
- **Pydantic v2:** Strict type validation
- **Typer + Rich:** CLI interface with beautiful output
- **pytest:** Test framework with 90%+ coverage
- **python-dateutil:** Relative date parsing

## Project Structure

```
weather-aware-scheduler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # Pydantic schemas
â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”œâ”€â”€ tools/           # Weather & calendar tools
â”‚   â”œâ”€â”€ graph/           # LangGraph nodes & edges
â”‚   â”œâ”€â”€ cli/             # Typer CLI
â”‚   â””â”€â”€ lib/             # Utilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/            # Component tests
â”‚   â”œâ”€â”€ integration/     # End-to-end tests
â”‚   â””â”€â”€ contract/        # Tool interface tests
â”œâ”€â”€ configs/             # Configuration files
â”œâ”€â”€ datasets/            # Test datasets
â”œâ”€â”€ scripts/             # Development scripts
â”œâ”€â”€ specs/               # Feature specifications
â””â”€â”€ docs/                # Documentation
```

## Development

### Run Tests

```bash
# All tests
pytest tests/ -v --cov=src

# Specific test type
pytest tests/unit/ -v
pytest tests/integration/ -v
```

### Run Dataset Evaluation

```bash
python scripts/replay_eval.py
```

**Expected:** 5/5 tests passing (100% success rate)

### Quick Development Cycle

```bash
bash scripts/dev_run.sh
```

Runs: setup â†’ tests â†’ CLI example â†’ visualization â†’ dataset replay

## Configuration

### Mock Mode (Default - No API Keys Required)

The system operates in mock mode by default, using:
- **Mock Weather Tool:** Keyword detection + time window rules
- **Mock Calendar Tool:** Predefined conflicts

Edit `configs/graph.config.yaml`:
```yaml
feature_flags:
  providers: "mock"  # No API keys required
```

### Real API Mode (Future)

To use real weather/calendar APIs, create `.env`:
```bash
OPENAI_API_KEY=sk-...
LANGSMITH_API_KEY=lsv2_...
```

Update `configs/graph.config.yaml`:
```yaml
feature_flags:
  providers: "mcp"  # Use real APIs
```

## Testing Strategy

### Test Pyramid

- **Unit Tests:** Parser, validator, time utils, formatters, mock tools
- **Integration Tests:** Full graph flows (sunny, rainy, conflict, missing info, failures)
- **Contract Tests:** Ensure mock and real tools have identical interfaces

### Golden Path Scenarios

1. **Sunny:** No conflicts, clear weather â†’ Direct success
2. **Rainy:** High rain probability â†’ Adjustment suggested
3. **Conflict:** Calendar clash â†’ 3 alternatives proposed
4. **Missing:** Incomplete input â†’ Clarification requested
5. **Failure:** Service unavailable â†’ Graceful degradation

## Documentation

- **[Quickstart Guide](specs/001-weather-aware-scheduler/quickstart.md)** - Get started in 5 minutes
- **[Feature Spec](specs/001-weather-aware-scheduler/spec.md)** - User stories and requirements
- **[Implementation Plan](specs/001-weather-aware-scheduler/plan.md)** - Technical architecture
- **[Tasks](specs/001-weather-aware-scheduler/tasks.md)** - Phase-by-phase breakdown

## Success Metrics

- âœ… **SC-002:** 100% success on 5 golden-path tests (achieved)
- âœ… **SC-006:** Request processing <3s (achieved in mock mode)
- âœ… **SC-008:** Conflict detection <1s (achieved)
- âœ… **Constitution:** Test suite <5s (achieved)
- ğŸ¯ **SC-004:** New developers understand flow in <5 min (visualizations ready)

## Contributing

This project follows test-driven development:
1. Write tests first (they should fail)
2. Implement minimum code to pass tests
3. Refactor while keeping tests green

## License

[Add license information]

## Status

**Current Phase:** User Stories 1-3 complete (MVP + Weather + Conflicts functional)

**Next:**
- âœ… Phase 6: Visualization (visualizer.py created, CLI command added)
- ğŸš§ Phase 7: Error handling refinement
- ğŸš§ Phase 8: Documentation polish

**Latest Test Results:** 78+ tests passing, 62-63% coverage

## Quick Reference

| Command | Purpose |
|---------|---------|
| `weather-scheduler schedule "..."` | Schedule a meeting |
| `weather-scheduler visualize` | Generate diagrams |
| `python scripts/replay_eval.py` | Run dataset tests |
| `pytest tests/ -v` | Run test suite |
| `bash scripts/dev_run.sh` | Full dev cycle |

---

Built with â¤ï¸ using LangGraph, Pydantic, and Test-Driven Development
